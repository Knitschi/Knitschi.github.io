<?xml version="1.0" encoding="UTF-8"?>
<add>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFBuildscripts</field>
    <field name="url">df/de6/_c_p_f_buildscripts.html</field>
    <field name="keywords"></field>
    <field name="text">The CPFBuildscripts package provides python scripts for simplifying the CMake calls of CPF projects The scripts can be used for configuring and building a CPFCMake project CMake projects usually require calls to CMake to generate makefiles and build the project Depending on the project the calls can require quite a list of arguments The python scripts provided by the CPFBuildscripts package use the knowledge about the structure of the CPF project to reduce the length of those commands It is therefore highly recommended to use the CPFBuildscripts package in combination with the CPFCMake package Index</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFCMake::CPFCMakeAPIDocs</field>
    <field name="url">d9/dfc/_c_p_f_c_make_a_p_i_docs.html</field>
    <field name="keywords"></field>
    <field name="text">API Documentation This page holds the documentation of the CMake functions that are provided to the users of the CPFCMake package This module provides the following functions cpfInitPackageProject() cpfAddPackage() This function is called at the beginning of a packages CMakeLists.txt file This function calls the project() function to create the package-level project It automatically reads the version number of the package from the packages git repository or a provided version file It also reads the directory name as the package name and returns it with parameter packageNameOut Parameter packageNameSpace is used in the following ways CPFCMake assumes that you use that namespace as the basic namespace for all your C++ files The name is used as a namespace in the packages generated C++ version header file As a namespace for the packages cmake target names As a part of the packages export macro which must be prepended to all exported classes and functions in a library The default package documentation page will generate a documentation of that namespace CI project vs package projects Adds a C++ package to a CPF project The function provides a large list of options that allow defining the features that the CPF package should provide A package consists of a main binary target that has the same name as the package and some helper binary targets for tests and test utilities Providing the function with optional arguments will switch on more of CPF s functionality like test-targets code-analysis packaging or documentation generation Custom targets Keyword Arguments PACKAGE_NAME The should be retrieved with the cpfInitPackageProject() function to make sure that it is the name of the current source directory This will be used as the name of the main binary target of the package BRIEF_DESCRIPTION A short description in one sentence about what the package does This is included in the generated documentation page of the package and in some distribution package types LONG_DESCRIPTION A longer description of the package This is included in the generated documentation page of the package and in some distribution package types HOMEPAGE A web address from where the source-code and/or the documentation of the package can be obtained This is required for Debian packages MAINTAINER_EMAIL An email address under which the maintainers of the package can be reached This is required for Debian packages PACKAGE_NAMESPACE cpfInitPackageProject TYPE The type of the main binary target of the package GUI_APP Executable with switched of console Use this for Qt applications with GUI CONSOLE_APP Console application LIB Library PUBLIC_HEADER All header files that declare functions or classes that are supposed to be used by consumers of a library package The public headers will automatically be put into binary distribution packages while header files in the PRODUCTION_FILES are not included PRODUCTION_FILES All files that belong to the production target If the target is an executable there should be a main.cpp that is used for the executable PUBLIC_FIXTURE_HEADER All header files in the fixture library that are required by external clients of the library If the fixture library is only used by this package this can be empty FIXTURE_FILES All files that belong to the test fixtures target TEST_FILES All files that belong to the test executable target LINKED_LIBRARIES The names of the library targets that are linked to the main binary target LINKED_TEST_LIBRARIES The names of the library targets that are linked to the test fixture library and the test executable Use this to specify dependencies of the test targets that are not needed in the production code like fixture libraries from other packages PLUGIN_DEPENDENCIES This keyword opens a sub-list of arguments that are used to define plugin dependencies of the package Multiple PLUGIN_DEPENDENCIES sub-lists can be given to allow having multiple plugin subdirectories The plugin targets are shared libraries that are explicitly loaded by the packages executables and on which the package has no link dependency If a target in the list does not exist when the function is called it will be silently ignored If a given target is an internal target an artificial dependency between the plugin target and the packages executables is created to make sure the plugin is compilation is up-to-date before the executable is build Adding this options makes sure that the plugin library is build before the executable and copied besides it in the PLUGIN_DIRECTORY Sub-Options PLUGIN_DIRECTORY A directory relative to the packages executables in which the plugin libraries must be deployed so they are found by the executable This if often a plugins directory PLUGIN_TARGETS The name of the targets that provide the plugin libraries DISTRIBUTION_PACKAGES This keyword opens a sub-list of arguments that are used to specify a list of packages that have the same content but different formats The argument can be given multiple times in order to define a variety of package formats and content types The argument takes two lists as sub-arguments A distribution package is created for each combination of the elements in the sub-argument lists For example argument DISTRIBUTION_PACKAGES_0 DISTRIBUTION_PACKAGE_CONTENT_TYPES BINARIES_USER_PORTABLE DISTRIBUTION_PACKAGE_FORMATS ZIP 7Z will cause the creation of a zip and a 7z archive that both contain the packages executables and all depended on shared libraries Adding another argument DISTRIBUTION_PACKAGES_1 DISTRIBUTION_PACKAGE_CONTENT_TYPES BINARIES_USER_NOSYSTEMLIBS DISTRIBUTION_PACKAGE_FORMATS DEB will cause the additional creation of a debian package that will not contain the dependencies marked as system libraries Sub-Options DISTRIBUTION_PACKAGE_CONTENT_TYPE BINARIES_DEVELOPER The package will include all package binaries header files and cmake config files for importing the package in another project This content type is supposed to be used for packages that distribute libraries BINARIES_USER listExcludedTargets The package will include the packages executables and shared libraries and all depended on shared libraries This is intended for packages that are delivered to the enduser who will not need header files etc The BINARIES_USER keyword can be followed by a list of depended on targets that shall not be included in the package This is usefull when the dependencies are provided by the systems package manager for example DISTRIBUTION_PACKAGE_FORMATS 7Z TGZ TXZ TZ ZIP Packs the distributed files into one of the following archive formats 7z tar.bz2 tar.gz tar.xz tar.Z zip DEB Creates a debian package deb file This will only be created when the dpkg tool is available DISTRIBUTION_PACKAGE_FORMAT_OPTIONS A list of keyword arguments that contain further options for the creation of the distribution packages SYSTEM_PACKAGES_DEB This is only relevant when using the DEB package format The option must be a string that contains the names and versions of the debian packages that provide the excluded shared libraries from the BINARIES_USER option E.g on which the package depends Example Here is an example of an CMakeLists.txt file for a library package</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFCMake::CPFConfiguration</field>
    <field name="url">d7/d8d/_c_p_f_configuration.html</field>
    <field name="keywords"></field>
    <field name="text">CPF configurations Add documentation of the default configurations example configs etc Maybe put the section about the configuration mechanism from the working with a cpf project in here</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFCMake</field>
    <field name="url">da/daa/_c_p_f_c_make.html</field>
    <field name="keywords"></field>
    <field name="text">CPFCMake The CPFCMake package implements a standardized C++ CMake project with additional CI functionality This package is the most basic component of the CMakeProjectFramework It helps with setting up a CMake based C++ project with extended functionality The package tries to solve the following problems Abstraction of common CMake code to a higher level CMakeProjectFramework projects are set up by using only a handfull of CMake functions This removes implementation details from the CMakeLists.txt files Providing a standardized directory structure for a C++ project Providing additional CI tasks like code-analysis or documentation-generation as custom targets Package-versioning based on version tags provided by the Git repository Modularisation of the code base into individual CMake packages Use of cmake configuration files which contain build configurations that outlive the deletion of the build directory and the CMakeCache.txt file Index API Documentation CPF configurations Custom targets Test targets Distribution packages Documentation generation Versioning</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFCMake::CPFCustomTargets</field>
    <field name="url">df/d03/_c_p_f_custom_targets.html</field>
    <field name="keywords"></field>
    <field name="text">Custom targets The build pipeline of a CPF project is implemented with CMake custom-targets In order to execute one of those tasks separately from the whole pipeline one has to build that target with the 3_Make py script The advantage of the custom-target mechanism is that the used build-system handles dependency issues rebuilding outdated targets and parallelizing task execution The availability of custom-targets in a CPF project depends on the projects configuration and its source files Most custom targets can be disabled via the configuration file This may be helpful if a custom implementation of the task is preferred Some tasks require a test executable which is only created if the package has a source file that defines the main function of a test-executable In some IDEs like Visual Studio or KDevelop targets are visualized and can be directly build from within the IDE This may sometimes be preferred to building the targets from the command line The following sections contain lists with the names of available custom targets The lists do not contain some private targets of the CPF that are only created as sub-steps of the targets that are of interest to the user A CPF project contains some targets that operate on the global level They either execute operations that can not be done for each package in separation or bundle up a certain kind of per-package targets In this case building the bundle target will simply build all per-package targets of that kind Here is a list of targets that can exist once per CPF project abi-compliance-checker acyclic ALL_BUILD clang-tidy distributionPackages doxygen globalFiles install opencppcoverage pipeline runAllTests runFastTests valgrind ZERO_CHECK Here is a list of targets that can exist once per CPF package abi-compliance-checker_ package clang-tidy_ package distributionPackages_ package install_ package opencppcoverage_ package package package _fixtures package _tests runAllTests_ package runFastTests_ package valgrind_ package This target bundles the abi-compliance-checker_ package targets The target checks that the projects target dependency graph is acyclic This target can be disabled with the CPF_ENABLE_ACYCLIC_TARGET variable This target builds all binary targets This target bundles the clang-tidy_ package targets This target bundles the distributionPackages_ package targets This target runs doxygen to generate the documentation for all owned packages It can be disabled with the CPF_ENABLE_DOXYGEN_TARGET variable This is only a file container target that does not execute any commands It holds all source files that are of global scope like tool configuration files global documentation etc This CMake standard target bundles the install_ package targets This target bundles the opencppcoverage_ package targets It also combines the temporary output of the opencppcoverage_ package targets into the final html report that can be found in the html output directory The top-level bundle target that will make sure that all other targets are built This target bundles the runAllTests_ package targets This target bundles the runFastTests_ package targets This target is not contained in the pipeline target which always builds the runAllTests target This target bundles the valgrind_ package targets A CMake default target that runs the CMake generate step This is a bundle target that runs the Abi-Compliance-Checker tool The target only exists for project configurations that use Gcc with debug flags and for shared library packages Report compatibility The basic functionality is to create html reports that compare the abi/api-compatibility of a previous libray package version with the current one The reporst are added to the project web-page To enable this the target must be able to download previously generated distribution packages of that package from the project web-page which must contain generated abi-dump files This complex requirement makes the target somewhat fragile This functionality can be disabled with the CPF_ENABLE_ABI_API_COMPATIBILITY_CHECK_TARGETS config variable Enforce compatibility You can also enable targets that will fail to build if abi or api compatibility is hurt by your current changes This option can be switched on in stable branches To do so use the CPF_CHECK_ABI_STABLE and CPF_CHECK_API_STABLE config variables This target only exists when compiling on Linux with the clang compiler It runs the clang-tidy tool on the packages source files The target can be disabled with the CPF_ENABLE_CLANG_TIDY_TARGET config variable Creates all distribution packages of the package A distribution package is a file that is distributed to users of the package This can be a zip file that contains the binaries or sources or an installer The target is only created if the addPackage() function has the DISTRIBUTION_PACKAGES argument set This target copies the packages binary files and public headers to the CMAKE_INSTALL_PREFIX package directory Check if this can still be used like the Linux make install step without breaking the creation of the distribution packages This target runs the test executable with OpenCppCoverage tool in order to create an html report that shows the code lines that are hit while running the tests This target will only exist for project configurations that use MSVC and will only run the tool when compiling in Debug configuration The target can be disabled with the CPF_ENABLE_OPENCPPCOVERAGE_TARGET config variable The main binary target of the package An additional library that can be used to share test utility code between packages It is only created if the addPackage() function has the FIXTURE_FILES and PUBLIC_FIXTURE_HEADER arguments set The test executable that belongs to the package This target is only created if the addPackage() function has the TEST_FILES argument set The executable should run automated tests when executed This target runs all the tests in the package _tests executable The target can be disabled with the CPF_ENABLE_RUN_TESTS_TARGET config variable This target runs all the tests in the package _tests executable that have either the word FastFixture or FastTests included in their name It is the the users responsibility to make sure that the tests with those names are really fast tests The purpose of the target is to provide a way of executing only tests that are run quickly an which are therefor useful when working in a tight test-driven development cycle The target can be disabled with the CPF_ENABLE_RUN_TESTS_TARGET config variable This target runs the test executable with the Valgrind tool which can help to detect memory leaks or undifined behavior The target only exists for project configurations that use Gcc or Clang with debug flags When this target is enabled you must also add the empty file Other/MyPackageValgrindSuppressions.supp file to all packages You can use this file to suppress false positives or unfixable issues that are found by Valgrind The target can be disabled with the CPF_ENABLE_VALGRIND_TARGET config variable</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFCMake::CPFDistributionPackages</field>
    <field name="url">d3/d64/_c_p_f_distribution_packages.html</field>
    <field name="keywords"></field>
    <field name="text">Distribution packages List all the currently available options for distribution packages</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFCMake::CPFDocumentationGeneration</field>
    <field name="url">dc/d68/_c_p_f_documentation_generation.html</field>
    <field name="keywords"></field>
    <field name="text">Documentation generation The CppCodeBase uses Doxygen to allow keeping the documentation right beside the code Doxygen has PlanUml enabled to allow embedding UML diagramms into to documentation Searching The search functionality is configured to use the server-side search approach as described here To make it work these points must be implemented The DoxygenConfig.txt must contain the correct value for the SEARCHENGINE_URL key This means that the url of the doxysearch.cgi file must be known and accessible before generating the documentation When the url of the documentation web-server changes this value must be changed too One can test if the cgi script works by entering This should return test succesfull The file search/search.js in the doxygen directory should also contain a correct linkt to the doxysearch.cgi file The web-server needs access to the right doxysearch.cgi file which is provided by Doxygen The doxysearch.cgi file must come from the same version of doxygen that is used to generate the html files and the doxysearch.db search database The webserver must be configured to use cgi scripts which is done by providing the serve-cgi-bin.conf file with the docker-image of the webserver The Dockerfile makes sure the file is copied into the container The help generation needs to execute the doxyindexer.exe to create the doxysearch.db serach-index for the doxysearch.cgi This is done in the python script 7_GenerateDocumentation.py The generated files must be copied to the documentation server container with the command The Dependency Graph Building the documentation target will also create two graphviz dot files for the dependency graph of the build C++ packages These graphs can be added to the projects documentation by adding the lines To the doxygen documentation The transitive reduced graph does not show direct dependencies when an indirect dependency exists This resulst in a cleaner graph which may sometimes be favoured to the complete graph</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFCMake::CPFCMakeTestTargets</field>
    <field name="url">d1/d4f/_c_p_f_c_make_test_targets.html</field>
    <field name="keywords"></field>
    <field name="text">Test targets Explain about test targets and the command line interface What options will the cpf give to the test exe in order to run fast tests and hand over test file directories Should it be possible for users to hand over their own options</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFCMake::CPFVersioning</field>
    <field name="url">d3/dae/_c_p_f_versioning.html</field>
    <field name="keywords"></field>
    <field name="text">Versioning This page contains information on how the versioning problem is handled in the CPF The source of a version number is the Git repository that contains the package or CI-repository During the generate step The CPF determines the current version number of each package by reading the release version tags of the repository This version number is then used by the CPF while creating distribution packages or in the production code Because of the fully automated versioning-pipeline of a CPF project developers can rely that builds of two different commits will never have the same version number All repositories that are used for a CPF project should have the release version tag 0.0.0 at one of their first commits The CPF requires at least one release version tag for its versioning mechanism to work Package ownership Format major minor patch Examples 1.0.1 0.0.99 The version tags must follow the given pattern in order to be recognized by the CPF Tags of this form are called release versions The CPF assumes that the commits with the release versions are the ones that are provided to clients Release version tags must be manually added to the repository when the developer deems a commit worthy to be published This can also be done via the build-job that is provided by the CPFMachines package Format major minor patch commit-nr hash Examples 1.0.1.13 0.0.99.1 In order to have different version numbers for each build the CPF will determine internal version numbers for each package whenever the generate step is executed The first three digits are derived from the latest release version number that can be seen from the current commit in one of its preceding commits The commit-nr is the number of commit that have been made since the commit that has the release version This allows to see if an internal version is older or younger than another internal version However the commit number alone does not make the version unique as the development could have branched since the last release version This could lead to two commits with the same version number For this reason the version number also contains the first digits of the commit hash This part will be as long as is needed to make it unique If the repository has local changes that have not yet been committed the optional postfix is added to the version number Dirty versions can in general not be rebuild by other developers and should therefore not be considered when trying to reproduce bugs For C++ packages the CPF will automatically generate a header file that contains the current version number The version can be obtained in the C++ code by using assuming that you have a package MyPackage with namespace mp The package version can be accessed in the CMakeLists.txt file of the package via the PROJECT_VERSION variable after the call of the cpfInitPackageProject() function if you want to generate your own version files In the CPF the version tags in the repository are also used to mark commits for which the pipeline target was successfully build This is only enforced in combination with the build-job that is provided by the CPFMachines package The build-job adds version tags after successfully building a commit When this policy is followed developers can quickly see which commits are worth checking out when they try to build older versions Version numbers are incremented by adding new release version tags to the repository This can either be done manually or by setting certain parameters to the build-job that is provided by the CPFMachines package The CPF assumes that release version tags are unique and ordered where smaller versions can only be followed by larger versions Before you manually add a release version tag you should also make sure that the pipeline target of that commit builds for all your supported configurations The build-job of the CPFMachines package will make sure that all of these requirements are met when incrementing version numbers Tagging a commit with a release version</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFMachines::CPFMachinesHookSetup</field>
    <field name="url">d7/d1f/_c_p_f_machines_hook_setup.html</field>
    <field name="keywords"></field>
    <field name="text">Automatic build-Job execution In day to day development it is practical when the build-job is run automatically whenever a developer pushes commits that should be integrated into a main branch of the project To enable that CPFJenkinsjob provides a python module that adds hook scripts to the repositories of your CPF-projects Add documentation for hook config and how to run it</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFMachines::CPFMachinesConfigFile</field>
    <field name="url">dd/d47/_c_p_f_machines_config_file.html</field>
    <field name="keywords"></field>
    <field name="text">The CPFMachines config file This page should provde information about all the parameter in the config file and what they do</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFMachines</field>
    <field name="url">d4/d62/_c_p_f_machines.html</field>
    <field name="keywords"></field>
    <field name="text">The CPF infrastructure includes a Jenkins server build slaves and web-servers for hosting the generated html pages This package provides a python script for setting up these servers The setup script requires a user provided configuration file and ssh access to all involved machines in the network The jenkins server can be configured to contain build-jobs for CPF based repositories For each of these projects a web-server is set up to host the html content that is generated by the CPF-projects build pipeline CPFMachines also provides a python script to deploy post-receive hooks to the cpf-project repositories Index How to setup the CPFMachines infrastructure Automatic build-Job execution The CPFMachines config file The jenkins build-job Machines and Container Manual Tests Current problems</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFMachines::CPFJenkinsJob</field>
    <field name="url">d7/dd1/_c_p_f_jenkins_job.html</field>
    <field name="keywords"></field>
    <field name="text">The jenkins build-job The jenkins job that is created by this package is a parameterized job The parameters can be used to execute multiple tasks on a CPF CI-project when the job is started via the Jenkins web-interface In this mode the job verifies the commits made to an integration branch by building the pipeline in all configurations merge the commits to a main branch and add a new internal-version tag to mark the commit as a successful build The project web-page will be updated with the latest version of the generated documentation This mode is the most common use case which should be triggered automatically when pushing commits to an integration branch of the build repository or one of the package repositories To do an integration job following parameters must be set branchOrTag origin/ name MainBranch task integration package If a package was changed this must be set to the package name Note that this must be a package that has its versioning handled by this build project all other can be left at their defaults In this mode the job will overwrite an existing internal version tag with a release version and rebuild the tagged commit to create build artifacts with the new version In this case you need to specify which part/digit of the version number should be incremented The less significant digits are reset to zero This will also update the projects web-page To create a release tag the following parameters must be set branchOrTag existing internal version-tag task one of incrementMajor incrementMinor incrementPatch all other can be left at their defaults Sometimes it is necessary to rebuild an already integrated commit in order to recreate build artifacts In this mode the job will not touch the version tags are changed This will also update the projects web-page When none of the jobs default parameters are changed it executes this task for the master branch To rebuild an existing commit the following parameters must be set branchOrTag version-tag all other can be left at their defaults When working on the CMakeProjectFramework itself it is sometimes useful to speed up the build-job by running it with a reduced workload and without modifying the repository Reducing the workload can be achieved by limiting the build to a special configuration and/or a special target In this case the following parameters must be set branchOrTag origin/ someBranch this should be the branch you work on task rebuild package The package you work on cpfConfiguration The configuration you are interested in target The target that is build After a successful build the jenkins job accumulates the html output from all configurations that are specified in the cpfCIBuildConfigurations.json file This is done because not all pipeline steps are available for all configurations For example the OpenCppCoverage report can only be generated on Windows and would miss if only the html-pages generated by a Linux configuration were published The Doxygen output can be created by all configurations and only one will be used in the accumulated html output The build-job will take the output from the last configuration in cpfCIBuildConfigurations.json file that generates it This may be important when you want to use the output of a special configuration Note that the configuration can influence the actual content of the documentation an example is the dependency graph in which the node shapes are different for shared and static libraries After accumulation the content of the html directory is copied to the web-server that belongs build project and can be accessed via that server In case you want to use a custom jenkins-job for publishing the webpage the CPF-job creates a build-artifact that contains the complete html directory</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFMachines::CPFMachinesContainer</field>
    <field name="url">d2/d67/_c_p_f_machines_container.html</field>
    <field name="keywords"></field>
    <field name="text">Machines and Container This section contains information about the machines and containers that are used to host the services that are needed to run the CPF build-pipeline Machine Setup This section describes which machines virtual machines and docker images are used to provide the development infrastructure for the CppCodeBase project Currently the setup includes the native Debian machine Feldrechengeraet This native machine runs three docker containers and a virtual Windows 10 machine The pipeline requires a windows based build-slave for Jenkins which is currently implemented as a virtual machine This machine has been configured manually and then a backup was made The backup is stored on the DatenBunker machine in the directory When the machine is started a script will automatically start the jenkins-slave service The medium-term goal is to remove the need for stored virtual machines and replace them with docker containers or virtual machines plus scripts that will do the machine setup This is supposed to improve the of the state of the build-machines because all the scripts that define them are contained in the repository Setting up the involved docker containers on Linux only requires the execution of the setupDockerContainer.sh script that can be found in the Infrastructure/DockerImages directory of the CppCodeBase Starting The Linux Docker Containers Currently the following machines are implemented as Linux based docker containers Documentation-Server Hosts the html pages that are generated by the build pipeline Currently this is the Doxygen documentation and a report from the OpenCppCoverage tool Jenkins-Master The machine that runs the Jenkins master and which is accessed to observer the build-pipeline Jenkins-Slave-Linux-0 The machine on which the actual builds of the Linux parts of the pipeline are done To set these machines up one needs a debian machine that has docker and git installed Then the CppCodeBase repository needs to be checked out in order to get the scripts One also needs to create a directory that will act as the home directory of the jenkins-master and is shared between the host and the container Currently this directory is hardcoded to which may need to be transformed to an script argument later The Infrastructure/DockerImages folder ccbContains the setupDockerContainer.sh script which does all the work Running it will require an working ssh connection to Datenbunker machine in order to setup the connection from the jenkins-master to the Datenbunker Network issues after running setupDockerContainer.sh Setting up the docker container changes the network settings of the host This may cause the the setupDockerContainer.sh script to fail when run multiple times in a row because connection to the internet can not be made Restarting the machine on which the script is run will solve the problem The Windows Slave There is a virtual machine that is used as a build-slave for the parts of the pipeline that must be build on Windows Whenever the container for the jenkins-master machine is re-built the connection information on the windows slave must be updated in the script that can be found on the windows-slave under C bat To update the information one must to login to jenkins from within the windows-slave and go to the page http://feldrechengeraet:8080/computer/jenkins-slave-windows-0 and copy the displayed secret into the batch file The batch file is run after each start of the windows-slave to reestablish the connection to the jenkins-master server The Jenkins Build Server The server is setup in the jenkins-master container Manual changes to the server via the web-interface are lost when the setupDockerContainer.sh is re-run This includes updates to plugins and jenkins itself Therefore non-experimental changes must be implemented by changing the files in the Infrastructure/DockerImages directory The web-page of the jenkins-server is here There is a post-commit hook that starts the pipeline The pipeline script builds the pipeline target on Linux and Windows in two configurations each The pipeline script collects html output of the pipeline and copies it to a web-server Git The Git repository is hosted on the DatenBunker machine in the directory The repository has a post-commit-hook that triggers the CppCodeBase_Build_Pipeline job on the Jenkins server Buy a more powerfull build-machine to reduce the overall pipeline time</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFMachines::CPFMachinesTests</field>
    <field name="url">de/d5a/_c_p_f_machines_tests.html</field>
    <field name="keywords"></field>
    <field name="text">Manual Tests This is a list of non automated tests This should only be a stopgap solution until they are automated Tests for versioning mechanics Running a successful build for an already pushed commit without a version tag should add an internl version tag to that commit Setting a release version tag should only work when the build is run on an internal version tag It should not be possible to create a release version tag that already exists When createin a release version the old internal version tag should be deleted Owned packages should be updated during the job run if new commits are available The job should fail when release tagging a non owned package</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFMachines::CPFMachinesProblems</field>
    <field name="url">d9/daf/_c_p_f_machines_problems.html</field>
    <field name="keywords"></field>
    <field name="text">Current problems Current Problems Machine names in the config.json files should be given in lower case only or problems with authenticating ssh server hosts can occur I was not able to make name resolution for windows machines in the host network work inside the docker containers The machine names for the windows build-slaves therefore must be given as ip addresses to allow the jenkins-master to reach them All machines should be within a trusted network as machine to machine communication is currently vulnerable to man in the middle attacks due to ignoring ssh host key checking When the machines access git repositories with the https protocol the passwords for these repositories are stored in plain text via the git credential store mechanism on the build slaves and the jenkins master machine Trouble Shooting The scripts in the CPFMachines package rely on a rather complex environment that must be setup manually A lot can go wrong here so here is a list of the most common problems Add list Notes With the current implementation parallel execution of the build-job is not possible This is because the version of a commit must stay the same while the job is running because the current version number is retrieved multiple times from the repository while the job is running Chaning the number in the meantime would lead to inconsistencies In the longer run it would be good to remove this limitation which could be a showstopper for highly frequented projects with long buildtimes Another problem is that we do not get a build for each push which makes it harder to pinpoint failed builds to a specific push/developer When using the CPF on Windows one may be hitting the 260 character limit for filenames On Windows 10 this limit can be switched off When building on older windows versions the user can try to reduce the length of his configuration names package names or ci-project name</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFMachines::CPFMachinesUsage</field>
    <field name="url">de/d15/_c_p_f_machines_usage.html</field>
    <field name="keywords"></field>
    <field name="text">How to setup the CPFMachines infrastructure Requirements At least one Linux Machine For Windows builds an additional Windows machine is required All machines must be in a local network that also contains the machine that runs the setup script Linux machines must be accessible via OpenSSH from the script-runner Linux machines must have docker installed Windows machines must be accessible via from the script-runner Windows machines must have the slave.jar copied to On windows build-slaves all tools that are required to run the CPF pipeline must be manually installed Setup infrastructure on fresh systems and complete the list This section has overlapping content with Setting up the CPF CI infrastructure We should merge them together Setting up the infrastructure After setting up the host-machines and getting the CPFMachines package you are ready to set up the servers that are involved in the CPF infrastructure If you want to use the infrastructure for multiple projects it is recommended that you create an extra CPF-project that holds the configuration files that are needed when running the setup scripts If you use the jenkins server for a single CPF-project you can add the configuration files to the global files if that project In both cases you will have to add the CPFMachines and most likely the CPFJenkinsjob package to the CPF-project that holds the configuration files Machine configuration Automatically add the content of the example config file here and manually add comments Running the script When you added the configuration file to the project you are ready to go Start the setup script by running in the root directory of your project This may take some time because some of the tools required for the pipeline build need to be compiled while setting up the docker container If everything went well the script ends with the output If you used the example config file you should now be able to access the Jenkins web-interface under http://MyMaster:8080 and the projects web-server under http://MyMaster:80 However the links to your projects will only work after running their build-jobs Change webserver file structure that multiple project pages can be served serachindex and an index.html in the base directory provides links to the single projects</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFTests</field>
    <field name="url">d5/d25/_c_p_f_tests.html</field>
    <field name="keywords"></field>
    <field name="text">A package that contains automated tests of the CMakeProjectFramework This package is not intended to be used by clients but rather implements mid- and high-level tests of the CPF As the tests in this package may test multiple packages at once and require some test setup-code to be run they where put in their own package Index</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFBasicConcepts</field>
    <field name="url">df/d2a/_c_p_f_basic_concepts.html</field>
    <field name="keywords"></field>
    <field name="text">Basic Concepts This pages describes the general ideas behind the CMakeProjectFramework It introduces some terms that are used throughout the documentation of this project The CPF follows the following key concepts Provide a Multi repository approach approach that allows the distribution of the code base over multiple repositories Define the build infrastructure completely in text-files that can be held in a repository This allows to quickly set up the infrastructure from a small amount of data The repository also provides the change history for the system Hide all CMake features behind a smaller declarative interface While cmake offers the functionality to solve all problems that occur when setting up a C++ code-base it is still quite some work to implement a solution The CPF wants to take that burden from its users while trying not to loose too much of the generality that a custom implementation would offer Keep files of three different live expectancies in three different directories Sources Configurations and generated files Keeping the generated files out of the source-tree has become the norm when using CMake This allows deleting the build-tree in order to guarantee a fresh system However in the standard CMake workflow the generated files also contain the manually defined project configuration within the CMakeCache.txt file For complex projects defining a configuration may take some time and deleting it together with the generated files is undesirable The CPF therefore puts the configuration information in a directory that is parallel to the build-* and It also allows defining multiple configurations at the same time Let the build system handle parallelism and up-to-date checks The CPF takes advantage of CMake s custom target mechanism This way the advanced tasks like test-runs code-analysis or documentation generation may be run in parallel and are only re-run when they are outdated Separate the CI-code from the production code by putting it into the CI-project repository Read more in CI project vs package projects In a CPF project the code that implements the CI-job is separated from the payload code The payload code can be split up into multiple packages To achieve this a CPF project has two CMake project levels The CI project is created in the root CMakeLists.txt file It provides the interface for the CI-job and the developers to build a set of packages and defines which configurations are build by the CI-system The project repository can also contain some higher level documentation that does not really belong to a single package but rather the group of packages that is owned by the CI project In the Sources subdirectory we have the package directories that contain the CMakeLists.txt files that define the package projects Each package creates one main binary that is supposed to be used by consumers This can be a library or an executable The package can also contain other arbitrary files like test-code image resources documentation and everything else that belongs to that piece of functionality For small code bases it is practical to keep all files in a single repository This avoids version conflicts and keeps the project nice and simple This means however that a change in a a lower lever library must always be pulled through* for all components that use the library A change in one cpp file will always trigger the build-job for the whole world One can imagine that this does not scale for a growing code-base The CPF therefore supports splitting up the code-base into multiple repositories and multiple build-jobs A CPF project can be started with the simple single repository and single build-job approach and later be split up in multiple repositories and multiple build-jobs One should be aware though that this introduces a noticeable increase in complexity The git work will increase and and version conflicts become possible Each CI-project repository defines a CI-build-job for multiple packages and each package can have its own repository which is recommended for packages that are consumed by multiple CI-projects Add a nice diagram of a monolithic project vs a distributed one A package can be owned by a CI project or be an external package If a package is owned by a CI project it means that this CI project is responsible for running the automated tests of the package and defining the officially supported build configurations of that the package When using the CPFMachines CI-job it will also be the entity that marks successful builds of owned packages with version tags An owned package can either be included in the CI projects git repository or it can be a git submodule with its own repository If it uses a git submodule it is called a loose owned package In this case it s version number can advance independently of the CI-repository Packages that are intended to be used in other projects should be loose packages with their own repository to allow other projects to include the package as a git submodule Loose packages lead to more and more complex git work meaning that more git operations will be necessary in the day to day work If packages are executables or not used in other projects it is therefore recommended to make them fixed packages and check them directly into the CI repository External packages are always git submodules The CI project s build-pipeline will not run the tests for these packages and exclude them when generating the documentation An external project should have another CI project that owns that package</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFProjectOrganisation</field>
    <field name="url">da/dca/_c_p_f_project_organisation.html</field>
    <field name="keywords"></field>
    <field name="text">Project Organization The CMakeProjectFramework master repository contains some of the documentation of the CMakeProjectFramework and provides the configuration for its official build job The master repository also serves as an for the packages that implement the various aspects of the CI system The packages are located in their own repositories and added as Git submodules to the master repository The packages are CPFCMake Contains CMake code that implements the additional CI tasks as custom targets CPFBuildscripts Provides python scripts that shorten the frequently used CMake calls CPFMachines Provides a python script that sets up a Jenkins-CI server and web-servers that provide CI-jobs for CPF-projects Package Dependencies Here is a simplified graph the illustrates the dependencies between the packages of the CMakeProjectFramework MyBuildRepository represents a project that uses the CMakeProjectFramework In the long run the dependency between CPFCMake and CPFMachines should be removed by making CPFCMake acquire all of the third party software via hunter instead of using the pre-installed software from the build slaves Dependencies The CppCodeBase cmake setup uses the hunter package manager to download and compile some of its dependencies During the configuration process you can define the HUNTER_ROOT directory that will be used by hunter to build and store some of the external dependencies If you you alread use hunter in another project make sure to set the variable to the same directory that is used by the other project in order to save compilation time and disk space for dependencies that are used by multiple projects</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFSettingUpACPFProject</field>
    <field name="url">d9/dc9/_c_p_f_setting_up_a_c_p_f_project.html</field>
    <field name="keywords"></field>
    <field name="text">Setting up a CPF project This section shows how to create a CPF based project Before doing so it is recommended that you get acquainted with the CMakeProjectFramework by building the example project as described on the Working with a CPF project page You should also have basic knowledge on how to work with Git This tutorial will walk you through the steps that are required to set-up a CPF project It starts with a very simple project that will then be extended to show more advanced features The goal of the tutorial is not only to create a new CPF project but also to explain some of it s details in the process If you already know your way around a CPF project and you just want to create a new one it is probably quicker if you simply copy the example project and remove the stuff that you do not need and rename the rest This tutorial assumes that you work on Windows with Visual Studio 2017 It also assumes that you can run the python scripts from the command line without prepending the extra python command If you want to run this tutorial on Linux you have to use the base Linux configuration in the configuration step instead of the Windows configuration You may also want to choose different names for your generated configurations like VS Gcc and VS2017-shared Gcc-shared Until not stated differently all commands should be executed in the projects root directory When running things on Linux there are a few differences Executables are generated in a slightly different directory with an additional bin subdirectory When running executables you have to prepend to the executable file At some places you may have to replace with You will have to replace commands for filesystem operations with their Linux equivalents First you need a Git repository that will hold your CI project For simplicity s sake we create the remote git repository on the same machine that we use for working on the cloned repository If you are familiar with setting up remote Git repositories you can also create it on a different machine Navigate to a directory that is suited for holding the repositories If you are on Windows make sure that it is a short path Because of the deep nested directory structure your CPF project may get into trouble with the 260 character path limit Create the directory MyCPFPRoject.git This will contain the remote repository Now open a command line and create the remote repository by executing the following commands Clone the remote repository by executing the following commands This will create the MyCPFProject directory This is the root directory of your project that we will use to add and edit source files In the next section we will add some basic files to that repository We now have the repository that will hold the files of our project First we add the basic files that are required for a CPF project You now have to create the files with the following content gitignore The file will make sure that directories and files that are generated by the CPF are ignored by git CMakeLists.txt This is the root CMakeLists.txt file of your CPF project include Including this file provides us with the CPF_MINIMUM_CMAKE_VERSION that is used in the next line Alternatively you can set your own minimum version that must be equal or higher then the version that is required by the CPF include This file provides the cpfInit() and cpfAddPackages() functions project(MyCPFProject) This creates the so called CI project cpfInit() This function from CPFCMake defines custom target properties set some CMake policies and search for some of the tools that are required by the CPF pipeline cpfAddPackages() This function adds the packages and global files to the CI project We add the basic packages cotire CPFCMake and CPFBuildscripts that are required for each CPF project As the packages are not yet in our repository we add them as git submodules in the next step After creating the files you should now commit them to the repository and add a first version tag The tag is required by the CPF to determine the version number of the packages If it is missing the CPF will later cause errors In the last section you added a CMakeLists.txt file that uses some functions from the CPFCMake package However you do not have those functions yet available in your project To change that we now add three packages as git submodules Open a command line and navigate to the Sources directory Then run The submodules CPFCMake and CPFBuildscripts are part of the CMakeProjectFramework Cotire is an internal third party dependency of the CPF that implements the automatic use of pre-compiled headers Now that we have acquired the CPF packages we can generate some more project files by running the generate step Run the following commands in the project root directory This will create three more files in the Sources directory Your project tree should now look like this The files cpfOwnedPackages.cmake documentation/DoxygenConfig.txt documentation/DoxygenLayout.xml and documentation/DoxygenStylesheet.xml need to be committed to the repository because they are generated when the generate step is run for the first time Run to add them to the repository Our CI project is now ready The only thing that is missing is some actual C++ code In order to add some payload code we add a package directly to the MyCPFProject repository This is a package that creates an executable Add the following file tree to the Sources directory The new files in the MyApp directory should have the following content CMakeLists.txt include(cpfAddPackage) Provides the cpfInitPackageProject() and cpfAddPackage() functions set( PACKAGE_NAMESPACE myapp ) The package namespace should be a short abbreviation of the package name It is used in multiple places As a part of the generated EXPORT MACRO that is used to publish symbol names in shared libraries This is only relevant if the package is a library As the namespace of exported CMake targets which is also only relevant for library packages It is recommended but not necessary that you use that name also as C++ namespace for all source files in the package The generated version header file at least uses this namespace set briefDescription and set longDescription The documentation pipeline will create an entry page for your packages documentation and fill it with the given strings cpfInitPackageProject() This function retrieves the current version number of your package from the repository It also reads the directory name and writes it into PACKAGE_NAME It also creates cmake and h files that provide CMake and C++ code with the current package version set PACKAGE_PRODUCTION_FILES A list with the currently available source files of the package This is a CMakeLists.txt file after all set PACKAGE_LINKED_LIBRARIES A list with targets on which the created package depends For now we have no dependencies so the list is empty cpfAddPackage This function creates the package project and all the custom targets that belong to a CPF package The bigger part of CPFCMake s functionality lies beneath this function function.h function.cpp This is a simple C++ file that implements a function It represents your C++ code The file includes the generated file cpfPackageVersion_MyApp.h header which provides function ma::getPackageVersion() that returns the current version number Note that the include directories and the project directory structure in the CPF is laid-out that all includes can uniformly be written with include package/file.h As recommended in the section for the CMakeLists.txt file we put our package functions into namespace ma main.cpp The CPF expects the main function to be in the file main.cpp In the case of a package that creates an executable the CPF internally creates a static library that contains all sources except the main.cpp file This is done to allow linking the complete functionality to a test executable that may also be created We now have to tell the CI-project that we added an owned package We do this by adding MyApp to the cpfOwnedPackages.cmake file Finally commit the new files to the repository by running With all the files in place we can now generate and build the project Note that you have to run a complete generate whenever you change the cpfOwnedPackages.cmake file A complete generate is executed when the configuration option is given to the 2_Generate.py script After the re-generate you can build and run the application Your actual version number will be different and depends on the current state of your repository You can read more about the CPF versioning here The pipeline target will also generate the documentation web-page To take a look at it open the file MyCPFProject/Generated/VS/html/doxygen/index.html In the Modules tab you will find a section for MyApp If you follow the link you can see the automatically generated package main page that contains the package descriptions from the CMakeLists.txt file The links in the lower part will not work because our project does not generate the linked pages For our first build we used one of the default configurations that come with the CPF Linux or Windows You can find more detailed information about configurations in the CPF here In this section we add our own default configuration to the project For demonstration purposes we change the configuration to build shared libraries instead of static libraries To create the new default configuration execute the following steps First we create a new configuration file that sets a different value for variable BUILD_SHARED_LIBS This variable is used to tell CMake to build shared libraries instead of static ones For now this has no effect because we have no library packages yet Instead of using the command line options you can also edit the configuration file with a text-editor which may be more comfortable if multiple values are changed Then we move the new configuration to the projects default configurations directory Sources/CIBuildConfigurations With the second configure step we use the new default configuration as our new local VS configuration When making changes to our configuration we have to do a fresh generate At the end we commit the new configuration file to the repository Developers can now use the default configuration of the project without the need of manually setting any CMake variables As your C++ project grows it will at some point become reasonable to split it into multiple libraries With the CPF we create libraries by adding a library package In this example we assume that our library will be used by other projects To Allow this we create a separate repository for the library package We then add this repository as a git submodule to our MyCPFProject repository If you do not know if a library will be shared between projects you can first add it directly to the CI repository to avoid the extra overhead of working with a git submodule If need be you can still put it in it s own repository later Create a new repository with the name MyLib using the same steps that you executed when creating the MyCPFProject repository You should end up with two empty repositories MyLib.git and MyLib Both lie besides the MyCPFProject.git and MyCPFProject directories We will first add some files to the MyLib repository and then add it as git submodule to MyCPFPRoject Add the following text-files to the MyLib repository and set the content as listed in the sections below CMakeLists.txt This file has some differences compared to the MyApp\CMakeLists.txt file We changed the name of the namespace and the description of the package We changed the TYPE argument in the cofAddPackage() call in order to create a library package We added the PUBLIC_HEADER argument to the cofAddPackage() call Libraries must provide public headers for consumers With the argument we can say which of our headers are supposed to be public Each library needs at least one public header or the project will fail to build function.h Note the use of the ML_EXPORT export macro The macro is provided by the ml_export.h header which is generated by CMake Export macros are normally only needed when building shared libraries However it is good practice to let the clients of a library decide whether they want to use it as a shared or static a library So better make sure that you always add the export macro to symbols that are intended to be used by clients If you do not add the macro in a shared library you will get linker errors function.cpp gitignore The package repository needs its own gitignore file Add one with the following content Now add commit and push all files in the MyLib repository We also add an initial version tag for the MyLib repository Now add MyLib as a loose owned package to MyCPFProject as a git submodule by running in the MyCPFProject/Sources directory This will yield a MyCPFPRoject/Sources/MyLib directory that contains the files that you created in the above section To finish the process edit some files in MyCPFPRoject cpfOwnedPackages.cmake We add the MyLib package as owned package to CI project by adding it in the cpfOwnedPackages.cmake file As the variable description states it is essential that MyLib is added to the list before MyApp MyApp/CMakeLists.txt To make the functionality of MyLib available in MyApp we have to add it to the linked libraries of MyApp MyApp/function.cpp We extend our original ma::function() to also call the ml::function() You now have to commit the changes to MyCPFProject and regenerate the make-files in order to finish adding the library package You can see that MyApp successfully calls the new function from MyLib Again your version numbers will be different MyLib has a different version then then MyApp because it lives in a different repository The CPF packages are designed to create an extra executable that runs automated tests for the packages production code This section will show you how to enable such a test executable for the MyLib package Add the new file MyLib_tests_main.cpp to a new Tests directory with the content In a real project you would probably use the main function that is provided by your test-framework instead of writing your own Note that we placed the file into the arbitrary Tests subdirectory which allows us to keep some order in our package Change the packages CMakeLists.txt file content to this We added two new lists PACKAGE_TEST_FILES and PACKAGE_LINKED_TEST_LIBRARIES and handed them to the cpfAddPackage() function The PACKAGE_TEST_FILES list should contain all source files that are used to build the test executable the PACKAGE_LINKED_TEST_LIBRARIES list can be used to add linked libraries that are only used by the test executable This could be a test-framework library for example In this example our test executable does not depend on any other library so we leave this empty You can now build and run your test executable by calling Somewhere in the output you should see the text output of the executable Note that tests will not be re-run if you execute the build command a second time You have to edit at least one source file of the package in order to outdate the test-run If you then rebuild the runAllTests target it will automatically create new binaries and run the tests with those For more information about the test targets of a CPF package see Test targets When writing a lot of automated tests it may become necessary to re-use test utility code from one package in another This could be fake or mock classes that you provide to replace the real objects in tests To make that possible the CPF can create an extra fixture library per package that can contain reusable test code To demonstrate this add two files function_fixture.h and function_fixture.cpp to the MyLib package with the following content and add them to the CMakeLists.txt file as shown below Sources/MyLib/Tests/function_fixture.h Note that the fixture library uses a different export macro then the production library Sources/MyLib/Tests/function_fixture.cpp Sources/MyLib/CMakeLists.txt For the fixture library we have to distinguish between public header files and other source files Add the new files to new list variables and as arguments to the cpfAddPackage() call as shown below Sources/MyLib/Tests/MyLib_tests_main.cpp Use the new function in the test code The fixture library is called package _fixtures and is automatically linked to the test executable If you need it in the tests of another package you have to add it to that packages PACKAGE_LINKED_TEST_LIBRARIES variable You can now compile and run your tests by calling One part of a CI pipeline is to create some sort of package that can be downloaded by the users of the software For applications this is usually some sort of installer which can be arbitrarily complex For libraries however this is usually just a ZIP archive that either holds the complete source code or the compiled artifacts and public headers In the CPF nomenclature we call these package files distribution packages in order to distinguish them from the CPF code packages in the Sources directory To enable creating distribution packages you have to add one more argument to the cpfAddPackage() function Sources/MyLib/CMakeLists.txt Note that the DISTRIBUTION_PACKAGES argument requires a list of nested key-word arguments due to the complexity of the option The options in this example will cause the creation of a developer binary package in the formats 7z and tar.gz Developer binary* means that the package will contain the compiled binaries and public headers For more information about creating other kinds of distribution packages read Distribution packages In order to create the specified packages run You should now have a directory MyCPFProject/Generated/VS/html/Downloads/MyLib/LastBuild with the two packages MyLib version Windows.dev and MyLib version Windows.dev The packages are added to the html directory so they can be directly downloaded from the projects web-page You now know the basics about setting up a CPF project If you still have open questions feel free to ask them on the projects github page</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFSettingUpTheInfrastructure</field>
    <field name="url">d9/d9a/_c_p_f_setting_up_the_infrastructure.html</field>
    <field name="keywords"></field>
    <field name="text">Setting up the CPF CI infrastructure This page contains a tutorial on how to use the scripts provided by the CPFMachines package to set up a continuous integration server infrastructure for you CPF projects The tutorial assumes that you are familiar with the Working with a CPF project and the Setting up a CPF project tutorials The tutorial also assumes that you have a CPF project provided by a remote git repository that can be pulled and pushed to via the SSH protocol It assumes you can clone that repository with The main functionality of the CPFMachines package is provided by a python script setup.py The script accesses the machines that belong to your environment over SSH in order to setup a Jenkins CI server build slaves and web-servers for your CPF project pages The script reads a configuration json file that contains values like machine access data the number of slaves that you desire etc This tutorial gives you an example on how to use create a correct configuration file provide the host machines for your servers and finally how to run the setup.py script In order to use the CPFMachines package you will have to create some custom configuration files that will define some basic properties of your server setup It is not necessary but recommended that you add these files to a git repository You can also use this repository to add other files with helper scripts and such that you use for handling your infrastructure We now add the CPFMachines package as a git submodule If you want your CPF project to be build on Linux and Window you have to at least provide one Debian 8.9 and one Windows 10 machine This tutorial assumes this minimalist setup The machines can be virtual or physical ones After you learned the basics you can still add more machines for in order to provide more build slaves If you do not care for Windows builds you can leave away the Windows machine If you only want to do Windows builds you will still need the Linux machine as it will host the jenkins and web server Get two fresh physical or virtual machines and install Windows 10 on one and Debian 8.9 on the other Both machines must be attached to the local network Create an account on the Debian machine that can be accessed via SSH Install docker on the debian machine Create an account on the Windows machine that can be accessed via SSH with the Bitvise SSH server Install build tools on the Windows machine Add more detailed instructions for the above steps Currently more manual steps are required The further steps assume that you can now run on the machine that you want to use to run the setup.py script Replace the user and machine names with the ones that you actually use We now add a configuration file to our MyCPFMachines repository The file contains information about our host machines and which build-jobs we want to have on the CI server Add a new file MyCPFMachines/MyConfig.json with the following content MyConfig.json With this configuration you will get the jenkins master server a jenkins linux agent and the web-server on the mydebianpc machine The mywindowspc will be used to run a jenkins windows agent Jenkins will be configured to have one build job MyCPFProject that will build your CPF project You can get more information about the configuration file here Adding passwords for your accounts to the config file is optional It may be saver to leave them out but it comes with the inconvenience that you have to re-enter them whenever you want to run the setup.py script Improve the setup script to allow the creation of an admin account on the first run We now have done all the manual preparations that are necessary to install the servers Running the script may take quite some time as some of the required tools are freshly compiled when running the script If the script fails to run successfully see if you can find the problem on the trouble shooting page here</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index::CPFWorkingWithCPF</field>
    <field name="url">d4/d37/_c_p_f_working_with_c_p_f.html</field>
    <field name="keywords"></field>
    <field name="text">Working with a CPF project This page provides practical information on how to do the day to day tasks that arise when working with a CPF project To illustrate the required steps the text refers to an example project that can be retrieved from Github Before you start you have to install the basic tools that are used by the CMakeProjectFramework Find out what must be installed by hand to compile the project Windows Visual Studio 2017 Git Python3 CMake 3.10 OpenCppCoverage optional Linux Gcc Git Python3 CMake 3.10 Clang optional Valgrind optional Git Python and CMake should be callable from the command-line PATH on Windows A CPF project must be based on a git repository The CMake code relies on it when determining package versions and when handling packages that can be contained in git submodules Because of the possibly contained submodules the repository must be cloned by using In order to build a freshly cloned CPF project four commands need to be exectuted Sadly this is a little more effort then the normal two steps generate and build that are used for a vanilla CMake project The steps are implemented with the following Python 3 scripts 0_CopyScritps.py Add commands to the projects root directory 1_Configure.py Set variables that determine the compiler etc 2_Generate.py Create the make-files for the project 3_Make.py Build the project If you have your operating system configured to run py files with python 3 you can omit the explicit call to python in the following command line examples If this is not the case make sure the python version call returns a a 3.X version On Linux you may need to use python3 instead of python In order to execute the copy step run in the project root directory This step copies some python scripts into the projects root directory The scripts are provided by the CPFBuildscripts package The scripts are only copied to shorten the command-line calls to these scripts while working with the project This step only needs to be executed once after cloning the repository In order to generate a configuration file run on Windows or on Linux in the project root directory The purpose of the configuration step is to create a config.cmake file that contains a set of CMake cache variables These variables determine things like the CMake generator or which custom targets are included in the pipeline The config file is used instead of the usual variable definitions in the CMake generate step The created file is located in the Configuration sub-directory The name of the configuration here VS or Gcc can be chosen freely The inherits option determines a base configuration from which the created file inherits default values for all required variables The base configuration can be provided by the CPFCMake package or the projects CIBuildConfigurations directory which is the common use case Some of the values in the configuration file like library locations or test file directories may be need to set to values that are specific to the machine onto which the project was cloned After running the script you have the chance to edit the default values in the created config file in order to change the values to something that is adequate for the local build On a CI server it may sometimes be useful to set non default values of variables directly with the command line call This can be done by adding options to the script call A project can have multiple configurations in parallel This can be achieved by running the 1_Configure.py script multiple times However short command versions for the following generate and build steps are only available when only one configuration is present The custom mechanism with the additional configuration file distinguishes the work-flow of the CPF project from the standard CMake command-line work-flow where the configure and generate step are executed at the same time A caveat of this approach is that configuration information is duplicated in the config.cmake file and the CMakeCache.txt file The developer has to remember that instead of editing the CMakeCache.txt file one now has to edit the config.cmake file and then re-execute the generate step The additional command line call may also come unexpected to developers who are used to work with normal CMake projects However CMake itself provides a similar three step work-flow when using the CMake-GUI application Here the user can also change values of variables in the CMakeCache.txt file before executing the generate step This indicates that there is a certain need for a three-step approach The CPF mechanism has some advantages over the two step work-flow which in my opinion outweigh the disadvantages Developers are relieved of remembering long lists of variable definitions that need to be typed whenever they need to re-generate the project Especially when working on cmake code it becomes often necessary to completely delete the build directory With the CPF mechanism the possibly manually edited configuration survives the deletion of the build directory The project can define default configurations that are used by the projects CI job This can be used to define with which compilers and on which platforms the project can be build To execute the generate step run on Windows or on Linux in the project root directory The generate step is the equivalent to the normally used cmake call It will create the build-directory Generated/VS that holds the created make-files for the generator that is set in the config file In the example this is the Visual Studio solution for the Windows case and the make files in the Linux case Note that during this step the hunter package manager may retrieve and compile external dependencies of the project The ACPFTestProject for example depends on the googletest package This means that this step may take quite some time when running it for the first time After the first run hunter-package build results are cached and subsequent executions of this step should be much faster When called with the configuration argument the script will delete the build directory before generating the make-files to guarantee that its content is clean The script can be called without the configuration argument In this case it will use the first configuration that is available in the Configuration directory It will also not delete the build directory before generating the files and do a faster incremental generate instead The script sometimes fails to clear the build directory See CPFBuildscripts issue 1 To execute a full build run on Windows or on Linux This will compile the binaries as well as executing extra pipeline tasks like running the tests do analysis generate the documentation or other steps that your project may have enabled via its configuration file Specifying the the cpf-configuration will cause a fresh rebuild If no cpf-configuration is specified the script will do an incremental build for the first configuration in the Generated directory With the target option one can specify which target should be build During development this is useful if only a smaller part of the pipeline should be executed Here is a list of available custom targets If the target option is omitted completely the script will only build the binary targets of the project The config option is only required for multi-configuration generators like Visual Studio If it is not specified the Debug configuration will be build Now that you have built the project it is time to take a look at the content of the test project The CMakeProjectFramework enforces a certain directory structure Here are the most important parts of that directory layout Note that depending on the configuration that you built not all of the shown directories and files will exist in your project Many of the displayed directories do not exist in a freshly cloned CPF project The ACPFTestProject directory is the root directory of the project This is the directory that you get when cloning a CPF project Most of the command line operations that are needed to handle the CPF project are executed in this directory The directory contains scripts to configure and build the project It also contains the Sources Configuration and Generated directories The Sources directory is stored in the repository while the other two are generated when setting up the project The Sources directory contains all the files that are checked into the repository After cloning a CPF repository this should be the only existing directory in the cpf-root-directory The Sources directory contains the root CMakeLists.txt file of the repository global files and directories for the packages that contain the payload code of the project There is a set of files that are in every CPF project CMakeLists.txt The root CMakeLists.txt file creates the CI-project This is the host project that contains the package projects that are created by the packages CMakeLists.txt files The creation of the CI-project is done by calling the cpfInit() function Packages are added by calling the cpfAddPackages() function Both are provided by the CPFCMake package cpfOwnedPackages.cmake This file defines a CMake cache variable that holds a list of package names that are owned by this CI-project Owned means that the CI-job that builds this repository is responsible for verifying that all automated checks for the package pass before it is marked with a version tag More information about package ownership can be found here DoxygenConfig.txt A configuration file for Doxygen that can be used to pass options to doxygen Note that this is not the final file that is used by doxygen More information about the documentation generation can be found here DoxygenLayout.xml A file which can be used to customize the look of the generated documentation CIBuildConfigurations This directory provides the CI job with information about the project configurations that should be build by the CI job These configurations are defined in files like VS.config.cmake which contain a set of CMake cache variables More information about the config file mechanism can be found here CIBuildConfigurations/cpfCiBuildConfigurations.json A file that contains a list of configurations that are build by the projects CI job This is only needed if the infrastructure provided by CPFMachines is used documentation The CPF adds this directory for global files that are involved in the documentation generation You can add other directories for global files to the Sources directory that do not contain packages but are rather used to order your global files like project wide documentation etc APackage A directory that contains a package The name of the package directory can be chosen by the user It also defines the name of the main library or executable that is created by this package A CPF project can have multiple package directories The package directory contains all source files that belong to the package These can hold the production code test code or the package documentation The package directory must contain a CMakeLists.txt file that calls the cpfInitPackageProject() and cpfAddPackage() functions The directory structure within the package directory can be chosen freely The relative directories of source files must be prepended when adding the files to the packages CMakeLists.txt file The Configuration directory contains CMake files that define the locally used configurations of the project This directory is generated by calling the 1_Configure.py script in the configuration step This directory is used to keep manually created project configurations out of the potentially short lived Generated directory The Generated directory contains all files that are generated by the generate- and build step All contents of that directory can be deleted without loosing any manual work However you will have to re-execute the generate and build step after deleting this directory The Generated directory contains one subdirectory for each configuration for which the generate step is executed The configuration directories are the CMake build directories that contain the usual CMake generated files as well as some special directories that are created by the CMake code of the CPF CPF specific build directory content html The primary output directory of the project It contains created distribution packages in the Downloads subdirectory The doxygen subdirectory contains the entry page of the generated project page which leads to the documentation and other optionally generated html pages like coverage report BuildStage This directory contains all the binaries that are generated when building the project When running an executable during debugging or automated testing it is run from within this directory _CPF A directory that is used for all internal files that are generated by the custom targets of the CPFCMake package If everything goes well the contents are only of interest when developing the CPFCMake package itself _pckg A directory that is used to accumulate the contents of the created distribution packages If everything goes well the contents are only of interest when developing the CPFCMake package itself The basic concepts page mentions that the CPF wants to separate CI-functionality from production code In the repository this is reflected by the two layers of CMakeLists.txt files The CI-project is defined by the root CMakeLists.txt file in the Sources directory The package projects are defined by the CMakeLists.txt files in the Sources package directories In the ACPFTestProject we have quite number of packages The packages APackage CPackage DPackage and EPackage are listed in the Sources/cpfOwnedPackages.cmake file which defines them as owned packages This means that it is this CI-project s responsibility to provide their official build-job that increment their version tags CPackage is the only fixed package which means that is in the same repository as the CI-project It is called fixed* because its version is fixed to the CI-projects version The other owned packages are loose because they are pulled in via the git-submodule mechanism which allows them to have their version incremented independently from the other packages The packages BPackage CPFBuildscripts CPFCMake CPFMachines FPackage GPackage and libSwitchWarningsOff are external packages External packages are always pulled in via the git-submodule mechanism Describe to most common git operations update of packages etc The cpfAddPackage function allows you to create binary packages for your library targets These packages contain cmake* files that can be used by other CMake based projects to consume your libraries with the find_package CONFIG function Currently binary packages with internal versions are not consumable by other cmake projects This is because the standard package files do not know how to handle the internal version number format of the CPF</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">todo</field>
    <field name="url">dd/da0/todo.html</field>
    <field name="keywords"></field>
    <field name="text">Todo List Page Automatic build-Job execution Add documentation for hook config and how to run it Page Basic Concepts Add a nice diagram of a monolithic project vs a distributed one Page CPF configurations Add documentation of the default configurations example configs etc Maybe put the section about the configuration mechanism from the working with a cpf project in here Page Current problems Add list Page Custom targets Check if this can still be used like the Linux make install step without breaking the creation of the distribution packages Page Distribution packages List all the currently available options for distribution packages Page How to setup the CPFMachines infrastructure Setup infrastructure on fresh systems and complete the list This section has overlapping content with Setting up the CPF CI infrastructure We should merge them together Automatically add the content of the example config file here and manually add comments Change webserver file structure that multiple project pages can be served serachindex and an index.html in the base directory provides links to the single projects Page Machines and Container Buy a more powerfull build-machine to reduce the overall pipeline time Page Setting up the CPF CI infrastructure Add more detailed instructions for the above steps Improve the setup script to allow the creation of an admin account on the first run Page Test targets Explain about test targets and the command line interface What options will the cpf give to the test exe in order to run fast tests and hand over test file directories Should it be possible for users to hand over their own options Page The CPFMachines config file This page should provde information about all the parameter in the config file and what they do Page Working with a CPF project Find out what must be installed by hand to compile the project Describe to most common git operations update of packages etc</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">bug</field>
    <field name="url">de/da5/bug.html</field>
    <field name="keywords"></field>
    <field name="text">Bug List Page Machines and Container Network issues after running setupDockerContainer.sh Setting up the docker container changes the network settings of the host This may cause the the setupDockerContainer.sh script to fail when run multiple times in a row because connection to the internet can not be made Restarting the machine on which the script is run will solve the problem Page Working with a CPF project The script sometimes fails to clear the build directory See CPFBuildscripts issue 1</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index</field>
    <field name="url">index.html</field>
    <field name="keywords"></field>
    <field name="text">Overview The CMakeProjectFramework The CMakeProjectFramework framework provides a solution for organizing a cross-platform Git and CMake based C++ software project At the low level it implements the execution of additional tasks like test-execution packaging or documentation generation This part is implemented with CMake and can be used without the higher level part which implements a Jenkins based CI-infrastructure that can be used to provide continuous integration services for a CPF project This project is work in progress Currently there are no automated tests that guarantee it to be in a working condition Index Basic Concepts Some fundamental design concepts of the project Working with a CPF project A tutorial on how to work with an existing CPF project Setting up a CPF project A tutorial on how to create your own CPF project Setting up the CPF CI infrastructure A tutorial on how to set up the build- and web-server for a CPF project Project Organization An overview of the packages that make up the CMakeProjectFramework CPFCMake More detailed information about the CPFCMake package CPFBuildscripts More detailed information about the CPFBuildscripts package CPFMachines More detailed information about the CPFMachines package CPFTests High-level tests for the complete CMakeProjectFramework Introduction When setting up a C++ Software project multiple tasks besides writing the actual C++ code arise Organizing the code-base into smaller packages Versioning of the packages Automatic acquisition of external software dependencies Setting up a build pipeline that also does additional tasks like running automated test static/dynamic analysis generate documentation etc Setting up a CI-server to run the pipeline Setting up a homepage for the project that hosts the documentation The CMakeProjectFramework tries to solve the above mentioned problems by using a combination of common open source tools from the C++ ecosystem The third party tools used by the CMakeProjectFramework are Git code versioning CMake meta-buildsystem Hunter package manager automatic dependency retrieval Cotire CMake module for pre-compiled headers Jenkins CI server Doxygen documentation generator clang-tidy static code analysis valgrind dynamic code analysis OpenCppCoverage measure test coverage Abi Compliance Checker visualize changes in the API/ABI between package versions Graphviz used to create a dependency graph of the project Using the CPF should relieve you of writing your own higher level CMake code or other additional scripts to implement pipeline tasks It can additionally provide a Jenkins CI infrastructure that is in the future completely generated from text-files Motivation In the smaller sized teams and companies that I have worked with improving the CI system was often a somewhat neglected part of the development Although the eco-system provides a number of open source tools that do the heavy lifting it is still quite some work to integrate all the tools into a full featured CI production system Implementing new features often had higher priority then the work on the infrastructure In the worst case this meant that the team had to work without a CI server or that the CI server only did the most basic jobs like building and packaging the project This situation bugged me and I started to implement a CI pipeline and infrastructure for my own C++ toy code-base At some point the project code started to get bigger then the C++ payload code I decided to pull out that project code into its own repository which now has become the CMakeProjectFramework At its current state the project is not battle tested As far as I know I am the only user and the project currently lacks the generality to be useful to everybody However I am interested in changing that and making the project useful to hobby developers or small teams that have not yet a CI pipeline in place If you are interested in using the project or joining its development feel free to contact me via Github Limitations The CMakeProjectFramework is currently only developed on Linux and Windows In the long run I would also like to support development on MacOS and cross-compiling to any other platform that is supported by CMake Setting up the CI infrastructure currently requires still a lot of manual work I do not know if custom pipeline steps can be easily added to the pipeline When using the CPF on windows one can easily run into the 260 character limit for filesystem paths If this is hit the only fix is to reduce the length of package names and the project name Similar projects Here is a non complete list of projects that have a similar goal BASIS</field>
  </doc>
</add>
